{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22dd0a7",
   "metadata": {},
   "source": [
    "# Q1.Simualte a DGP where the outcome of interest depends on a randomly assigned treatment and some observed covariates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cd8172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import graphviz as gr\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea72f875",
   "metadata": {},
   "source": [
    "## · DGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdbe1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_variance(data, ddof=0):\n",
    "    n = len(data)\n",
    "    mean = sum(data) / n\n",
    "    return sum((x - mean) ** 2 for x in data) / (n - ddof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "855ed369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_generate_cov(dim):\n",
    "    acc  = []\n",
    "    for i in range(dim):\n",
    "        row = np.ones((1,dim)) * corr\n",
    "        row[0][i] = 1\n",
    "        acc.append(row)\n",
    "    return np.concatenate(acc,axis=0)\n",
    "\n",
    "def fn_generate_multnorm(nobs,corr,nvar):\n",
    "\n",
    "    mu = np.zeros(nvar)\n",
    "    std = (np.abs(np.random.normal(loc = 1, scale = .5,size = (nvar,1))))**(1/2)\n",
    "    acc = []\n",
    "    for i in range(nvar):\n",
    "        acc.append(np.reshape(np.random.normal(mu[i],std[i],nobs),(nobs,-1)))\n",
    "    \n",
    "    normvars = np.concatenate(acc,axis=1)\n",
    "\n",
    "    cov = fn_generate_cov(nvar)\n",
    "    C = np.linalg.cholesky(cov)\n",
    "\n",
    "    Y = np.transpose(np.dot(C,np.transpose(normvars)))\n",
    "\n",
    "    return Y\n",
    "\n",
    "def fn_randomize_treatment(N,p=0.5):\n",
    "    treated = random.sample(range(N), round(N*p))\n",
    "    return np.array([(1 if i in treated else 0) for i in range(N)]).reshape([N,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e57d33c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_generate_data(tau,N,p,p0,corr,conf = True,flagX = False):\n",
    "    \"\"\"\n",
    "    p0(int): number of covariates with nonzero coefficients\n",
    "    \"\"\"\n",
    "    nvar = p+2 # 1 confounder and variable for randomizing treatment\n",
    "    corr = 0.5 # correlation for multivariate normal\n",
    "\n",
    "    if conf==False:\n",
    "        conf_mult = 0 # remove confounder from outcome\n",
    "    else:\n",
    "        conf_mult = 0\n",
    "        \n",
    "    allX = fn_generate_multnorm(N,corr,nvar)\n",
    "    W0 = allX[:,0].reshape([N,1]) # variable for RDD assignment\n",
    "    C = allX[:,1].reshape([N,1]) # confounder\n",
    "    X = allX[:,2:] # observed covariates\n",
    "    \n",
    "    T = fn_randomize_treatment(N) # choose treated units\n",
    "    err = np.random.normal(0,1,[N,1])\n",
    "    beta0 = np.random.normal(5,5,[p,1])\n",
    "    \n",
    "    beta0[p0:p] = 0 # sparse model\n",
    "    Yab = tau*T+X@beta0+conf_mult*0.6*C+err\n",
    "    if flagX==False:\n",
    "        return (Yab,T)\n",
    "    else:\n",
    "        return (Yab,T,X)\n",
    "    \n",
    "    # regression discontinuity\n",
    "#     W = W0 + 0.5*C+3*X[:,80].reshape([N,1])-6*X[:,81].reshape([N,1])\n",
    "#     treated = 1*(W>0)\n",
    "#     Yrdd = 1.2* treated - 4*W + X@beta0 +0.6*C+err\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e07fd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_tauhat_means(Yt,Yc):\n",
    "    nt = len(Yt)\n",
    "    nc = len(Yc)\n",
    "    tauhat = np.mean(Yt)-np.mean(Yc)\n",
    "    se_tauhat = (np.var(Yt,ddof=1)/nt+np.var(Yc,ddof=1)/nc)**(1/2)\n",
    "    return (tauhat,se_tauhat)\n",
    "\n",
    "def fn_bias_rmse_size(theta0,thetahat,se_thetahat,cval = 1.96):\n",
    "    \"\"\"\n",
    "    theta0 - true parameter value\n",
    "    thetatahat - estimated parameter value\n",
    "    se_thetahat - estiamted se of thetahat\n",
    "    \"\"\"\n",
    "    b = thetahat - theta0\n",
    "    bias = np.mean(b)\n",
    "    rmse = np.sqrt(np.mean(b**2))\n",
    "    tval = b/se_thetahat # paramhat/se_paramhat H0: theta = 0\n",
    "    size = np.mean(1*(np.abs(tval)>cval))\n",
    "    # note size calculated at true parameter value\n",
    "    return (bias,rmse,size)\n",
    "\n",
    "def fn_run_experiments(tau,Nrange,p,p0,corr,conf,flagX=False):\n",
    "    \"\"\"\n",
    "    tau - treatment effect\n",
    "    Nrange - range of sample sizes\n",
    "    p - number of covariates generated\n",
    "    p0 - number of covariates included\n",
    "    corr - correlation between covariates\n",
    "    conf - confounder\n",
    "    \"\"\"\n",
    "    n_values = []\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    lb = []\n",
    "    ub = []\n",
    "    for N in tqdm(Nrange):\n",
    "        n_values = n_values + [N]\n",
    "        if flagX==False:\n",
    "            #No X's included\n",
    "            Yexp,T = fn_generate_data(tau,N,p,p0,corr,conf,flagX)\n",
    "            Yt = Yexp[np.where(T==1)[0],:]\n",
    "            Yc = Yexp[np.where(T==0)[0],:]\n",
    "            tauhat,se_tauhat = fn_tauhat_means(Yt,Yc)            \n",
    "        elif flagX==1:\n",
    "            # use the right covariates in regression\n",
    "            Yexp,T,X = fn_generate_data(tau,N,p,p0,corr,conf,flagX)\n",
    "            Xobs = X[:,:p0]\n",
    "            covars = np.concatenate([T,Xobs],axis = 1)\n",
    "            mod = sm.OLS(Yexp,covars)\n",
    "            res = mod.fit()\n",
    "            tauhat = res.params[0]\n",
    "            se_tauhat = res.HC1_se[0]\n",
    "        elif flagX==2:\n",
    "            # use some of the right covariates and some \"wrong\" ones\n",
    "            Yexp,T,X = fn_generate_data(tau,N,p,p0,corr,conf,flagX)\n",
    "            Xobs1 = X[:,:np.int(p0/2)]\n",
    "            Xobs2 = X[:,-np.int(p0/2):]\n",
    "            covars = np.concatenate([T,Xobs1,Xobs2],axis = 1)\n",
    "            mod = sm.OLS(Yexp,covars)\n",
    "            res = mod.fit()\n",
    "            tauhat = res.params[0]\n",
    "            se_tauhat = res.HC1_se[0]\n",
    "            \n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]    \n",
    "        lb = lb + [tauhat-1.96*se_tauhat]\n",
    "        ub = ub + [tauhat+1.96*se_tauhat]\n",
    "        \n",
    "    return (n_values,tauhats,sehats,lb,ub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a879ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "n = 1000\n",
    "T = np.random.binomial(1, 0.5, n)\n",
    "X1 = np.random.normal(20,5,n)\n",
    "X2 = np.random.normal(20,5,n)\n",
    "X3 = np.random.normal(20,5,n)\n",
    "Y = np.random.normal(1 +1*T+1*X1+1*X2+1*X3, 5).astype(int)\n",
    "data1 = pd.DataFrame(dict(T = T,\n",
    "                            X1 = X1,\n",
    "                            X2 = X2,\n",
    "                            X3 = X3,\n",
    "                            Y = Y))\n",
    "data1.to_csv(\"data-covariates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738cbf96",
   "metadata": {},
   "source": [
    "## · DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ae559cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (20211204.2007)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"278pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 278.00 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-112 274,-112 274,4 -4,4\"/>\n",
       "<!-- X1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>X1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">X1</text>\n",
       "</g>\n",
       "<!-- Y -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Y</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"135\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"135\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Y</text>\n",
       "</g>\n",
       "<!-- X1&#45;&gt;Y -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>X1&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45.81,-76.81C63,-65.67 88.62,-49.06 107.99,-36.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.92,-39.43 116.4,-31.05 106.11,-33.56 109.92,-39.43\"/>\n",
       "</g>\n",
       "<!-- X2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>X2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">X2</text>\n",
       "</g>\n",
       "<!-- X2&#45;&gt;Y -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>X2&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M107.35,-72.76C111.71,-64.28 117.15,-53.71 122.04,-44.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"125.23,-45.64 126.7,-35.15 119.01,-42.44 125.23,-45.64\"/>\n",
       "</g>\n",
       "<!-- X3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>X3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">X3</text>\n",
       "</g>\n",
       "<!-- X3&#45;&gt;Y -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>X3&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M162.65,-72.76C158.29,-64.28 152.85,-53.71 147.96,-44.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"150.99,-42.44 143.3,-35.15 144.77,-45.64 150.99,-42.44\"/>\n",
       "</g>\n",
       "<!-- T -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>T</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"243\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"243\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">T</text>\n",
       "</g>\n",
       "<!-- T&#45;&gt;Y -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>T&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M224.19,-76.81C207,-65.67 181.38,-49.06 162.01,-36.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"163.89,-33.56 153.6,-31.05 160.08,-39.43 163.89,-33.56\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fceda07c3d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = gr.Digraph()\n",
    "\n",
    "g.edge(\"X1\", \"Y\")\n",
    "g.edge(\"X2\", \"Y\")\n",
    "g.edge(\"X3\", \"Y\")\n",
    "g.edge(\"T\", \"Y\")\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa8922f",
   "metadata": {},
   "source": [
    "## · Run a Monte Carlo experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7641e68",
   "metadata": {},
   "source": [
    "### a. You do not control for any covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a7d23",
   "metadata": {},
   "source": [
    "$Y_i = \\tau*T_i+e_i$\n",
    "\n",
    "$e_i \\sim N(0,\\sigma^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4166fe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 198/198 [00:00<00:00, 448.04it/s]\n"
     ]
    }
   ],
   "source": [
    "tau=2\n",
    "corr = .5\n",
    "conf=False\n",
    "p = 10\n",
    "p0 = 0\n",
    "Nrange = range(10,1000,5)\n",
    "(nvalues,tauhats,sehats,lb,ub) = fn_run_experiments(tau,Nrange,p,p0,corr,conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71191721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2000/2000 [00:00<00:00, 2547.38it/s]\n",
      "100%|██████████████████████████████████████| 2000/2000 [00:10<00:00, 182.98it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 2000\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    for r in tqdm(range(R)):\n",
    "        Yexp,T = fn_generate_data(tau,N,10,0,corr,conf)\n",
    "        Yt = Yexp[np.where(T==1)[0],:]\n",
    "        Yc = Yexp[np.where(T==0)[0],:]\n",
    "        tauhat,se_tauhat = fn_tauhat_means(Yt,Yc)\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d21dd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=-0.0013812918129380175, RMSE=0.19755259359772204, size=0.049\n",
      "N=1000: bias=0.0002580789609925882, RMSE=0.06216041031105066, size=0.0445\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c424bf",
   "metadata": {},
   "source": [
    "### b.You control for all the covariates that affect the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d22bb3e",
   "metadata": {},
   "source": [
    "$Y_i = \\tau*T_i+\\beta'*X_i+e_i$\n",
    "\n",
    "$e_i \\sim N(0,\\sigma^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "893d2307",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau=2\n",
    "corr = .5\n",
    "conf=False\n",
    "p = 10\n",
    "p0 = 10\n",
    "flagX=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd37c6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2000/2000 [00:00<00:00, 2460.19it/s]\n",
      "100%|██████████████████████████████████████| 2000/2000 [00:10<00:00, 184.44it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 2000\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    for r in tqdm(range(R)):\n",
    "        Yexp,T = fn_generate_data(tau,N,10,10,corr,conf)\n",
    "        Yt = Yexp[np.where(T==1)[0],:]\n",
    "        Yc = Yexp[np.where(T==0)[0],:]\n",
    "        tauhat,se_tauhat = fn_tauhat_means(Yt,Yc)\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0285b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=-0.058455335977126655, RMSE=8.010988468295231, size=0.0485\n",
      "N=1000: bias=-0.07220545318704916, RMSE=2.62637391760449, size=0.049\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5599e6b3",
   "metadata": {},
   "source": [
    "## · Example of a real-life situation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb146f85",
   "metadata": {},
   "source": [
    "X(covariates): student's age, gender,family background \n",
    "\n",
    "T: small size class(<=20) or not\n",
    "\n",
    "Y: student's performance\n",
    "\n",
    "I want to study the effect of small size class on students' performance. To rule out the effects of other covariates on students' performance , I should control for those variables, such as student's age, gender and family background."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d3c30",
   "metadata": {},
   "source": [
    "# Q2. Simulate a DGP with a confounder (common cause)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ec53d",
   "metadata": {},
   "source": [
    "$Y_i = \\tau*T_i+0.8*C+e_i$  C: Confounder\n",
    "\n",
    "$T_i =0.5*C$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158a2788",
   "metadata": {},
   "source": [
    "## · DGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0182741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_generate_data_conf(tau,N,p,corr):\n",
    "\n",
    "    nvar = p+1 \n",
    "    corr = 0.5 \n",
    " \n",
    "    X = fn_generate_multnorm(N,corr,nvar)\n",
    "    C = X[:,1].reshape([N,1])  \n",
    "    T = fn_randomize_treatment(N) \n",
    "    err = np.random.normal(0,1,[N,1])\n",
    "    Yab = tau*T+0.8*C+err\n",
    "    Tab = T+0.5*C\n",
    "    \n",
    "    return (Yab,Tab,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecd6fce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 198/198 [00:00<00:00, 459.28it/s]\n"
     ]
    }
   ],
   "source": [
    "random.seed(10)\n",
    "\n",
    "tau = 2\n",
    "corr = 0.5\n",
    "p = 10\n",
    "conf=True\n",
    "Nrange = range(10,1000,5)\n",
    "(nvalues,tauhats,sehats,lb,ub) = fn_run_experiments(tau,Nrange,p,p0,corr,conf)\n",
    "\n",
    "Y,T,C = fn_generate_data_conf(tau,N,p,corr)\n",
    "data = np.concatenate([Y,T,C],axis = 1)\n",
    "data = pd.DataFrame(data)\n",
    "data.columns = ['Y', 'T', 'Confounder']\n",
    "data.to_csv('data-confounder.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a645c2",
   "metadata": {},
   "source": [
    "## · DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8763294f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (20211204.2007)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"89pt\" height=\"188pt\"\n",
       " viewBox=\"0.00 0.00 89.00 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-184 85,-184 85,4 -4,4\"/>\n",
       "<!-- T -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>T</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">T</text>\n",
       "</g>\n",
       "<!-- Y -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Y</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"54\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"54\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Y</text>\n",
       "</g>\n",
       "<!-- T&#45;&gt;Y -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>T&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M33.4,-72.41C36.51,-64.34 40.33,-54.43 43.83,-45.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47.13,-46.55 47.46,-35.96 40.6,-44.03 47.13,-46.55\"/>\n",
       "</g>\n",
       "<!-- C -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>C</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"54\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"54\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">C</text>\n",
       "</g>\n",
       "<!-- C&#45;&gt;T -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>C&#45;&gt;T</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M47.6,-144.41C44.49,-136.34 40.67,-126.43 37.17,-117.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"40.4,-116.03 33.54,-107.96 33.87,-118.55 40.4,-116.03\"/>\n",
       "</g>\n",
       "<!-- C&#45;&gt;Y -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>C&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M57.65,-143.91C59.68,-133.57 61.98,-120.09 63,-108 64.34,-92.06 64.34,-87.94 63,-72 62.28,-63.5 60.93,-54.31 59.49,-46.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.91,-45.29 57.65,-36.09 56.03,-46.56 62.91,-45.29\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fceda10d910>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = gr.Digraph()\n",
    "\n",
    "g.edge(\"T\", \"Y\")\n",
    "g.edge(\"C\", \"Y\")\n",
    "g.edge(\"C\", \"T\")\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec7e6a7",
   "metadata": {},
   "source": [
    "## · Run a Monte Carlo experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1172e1",
   "metadata": {},
   "source": [
    "### a.You fail to control for the confounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52d51039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2000/2000 [00:01<00:00, 1216.36it/s]\n",
      "100%|██████████████████████████████████████| 2000/2000 [00:11<00:00, 169.84it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 2000\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    for r in tqdm(range(R)):\n",
    "        Y,T,C = fn_generate_data_conf(tau,N,p,corr)\n",
    "        covars = np.concatenate([T],axis = 1)\n",
    "        mod = sm.OLS(Y,covars)\n",
    "        res = mod.fit()\n",
    "        tauhat = res.params[0]\n",
    "        se_tauhat = res.HC1_se[0]\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59e24b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=-0.1254042807202189, RMSE=0.17665819449289807, size=0.218\n",
      "N=1000: bias=-0.13123813341968138, RMSE=0.14085070684205012, size=0.866\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9961a31d",
   "metadata": {},
   "source": [
    "### b.You do control for the confounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3873857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2000/2000 [00:01<00:00, 1356.00it/s]\n",
      "100%|██████████████████████████████████████| 2000/2000 [00:11<00:00, 167.85it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 2000\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    for r in tqdm(range(R)):\n",
    "        Y,T,C = fn_generate_data_conf(tau,N,p,corr)\n",
    "        covars = np.concatenate([T,C],axis = 1)\n",
    "        mod = sm.OLS(Y,covars)\n",
    "        res = mod.fit()\n",
    "        tauhat = res.params[0]\n",
    "        se_tauhat = res.HC1_se[0]\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "        \n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7326f2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=0.0012971666607536178, RMSE=0.1384009918016348, size=0.0555\n",
      "N=1000: bias=-0.0016538399535356801, RMSE=0.04551121044391871, size=0.053\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc8d88d",
   "metadata": {},
   "source": [
    "## · Example of a real-life situation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03dfe2",
   "metadata": {},
   "source": [
    "C: monthly salary\n",
    "\n",
    "T: have a car or not\n",
    "\n",
    "Y: monthly expense\n",
    "\n",
    "I want to study the effect of having a car or not on monthly expense. Since montly salary is a confounder, which causes both having a car or not and monthly expense, I control for monthly salary to ensure the independence between T and Y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8355104b",
   "metadata": {},
   "source": [
    "# Q3.Simulate a DGP with selection bias into the treatment (variable in between the path from thetreatment to the outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b8b17",
   "metadata": {},
   "source": [
    "$Y_i = \\tau*T_i+e_i$  \n",
    "\n",
    "$X_i =0.5*Y_i+0.4*T_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad62cd4",
   "metadata": {},
   "source": [
    "## · DGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a59ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_generate_data_sele(tau,N,p,corr):\n",
    "\n",
    "    nvar = p+1 # 1 for selection bias\n",
    "    corr = 0.5 # correlation for multivariate normal\n",
    " \n",
    "    allX = fn_generate_multnorm(N,corr,nvar)\n",
    "     \n",
    "    T = fn_randomize_treatment(N) # choose treated units\n",
    "    err = np.random.normal(0,1,[N,1])\n",
    "    U = np.random.normal(0,1,[N,1])\n",
    "    Yab = tau*T+err\n",
    "    Xab = 0.4*T+0.5*Yab+U\n",
    "\n",
    "    return (Yab,T,Xab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cff3d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 2\n",
    "corr = .5\n",
    "p = 10\n",
    "N = 1000\n",
    "\n",
    "Y,T,X = fn_generate_data_sele(tau,N,p,corr)\n",
    "\n",
    "data = np.concatenate([Y,T,X],axis = 1)\n",
    "data = pd.DataFrame(data)\n",
    "data.columns = ['Y', 'T', 'X']\n",
    "data.to_csv('data-selection.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aebfb0",
   "metadata": {},
   "source": [
    "## · DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ba84ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (20211204.2007)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"90pt\" height=\"188pt\"\n",
       " viewBox=\"0.00 0.00 90.00 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-184 86,-184 86,4 -4,4\"/>\n",
       "<!-- T -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>T</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">T</text>\n",
       "</g>\n",
       "<!-- X -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- T&#45;&gt;X -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>T&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M23.75,-143.89C21.95,-133.54 19.91,-120.06 19,-108 17.8,-92.04 17.8,-87.96 19,-72 19.64,-63.52 20.84,-54.34 22.12,-46.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"25.58,-46.55 23.75,-36.11 18.68,-45.41 25.58,-46.55\"/>\n",
       "</g>\n",
       "<!-- Y -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Y</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"55\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"55\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">Y</text>\n",
       "</g>\n",
       "<!-- T&#45;&gt;Y -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>T&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M33.64,-144.41C36.91,-136.22 40.94,-126.14 44.62,-116.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47.95,-118.05 48.41,-107.47 41.45,-115.45 47.95,-118.05\"/>\n",
       "</g>\n",
       "<!-- Y&#45;&gt;X -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>Y&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M48.36,-72.41C45.09,-64.22 41.06,-54.14 37.38,-44.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"40.55,-43.45 33.59,-35.47 34.05,-46.05 40.55,-43.45\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fce98091fd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = gr.Digraph()\n",
    "\n",
    "g.edge(\"T\", \"X\")\n",
    "g.edge(\"T\", \"Y\")\n",
    "g.edge(\"Y\", \"X\")\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77765f63",
   "metadata": {},
   "source": [
    "## · Run a Monte Carlo experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd9e3f",
   "metadata": {},
   "source": [
    "### a. You control for the variable in between the path from cause to effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a8a5d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2000/2000 [00:01<00:00, 1168.81it/s]\n",
      "100%|██████████████████████████████████████| 2000/2000 [00:11<00:00, 167.18it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 2000\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    for r in tqdm(range(R)):\n",
    "        Yexp,T,Xexp = fn_generate_data_sele(tau,N,p,corr)\n",
    "        covars = np.concatenate([T,Xexp],axis = 1)\n",
    "        mod = sm.OLS(Yexp,covars)\n",
    "        res = mod.fit()\n",
    "        tauhat = res.params[0]\n",
    "        se_tauhat = res.HC1_se[0]\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "        \n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c551ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=-0.5704068073396732, RMSE=0.5948695742293214, size=0.914\n",
      "N=1000: bias=-0.5603346292305649, RMSE=0.5628194122803005, size=1.0\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b4419d",
   "metadata": {},
   "source": [
    "### b. You do not control for the variable in between the path from cause to effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f4f3e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2000/2000 [00:00<00:00, 2673.19it/s]\n",
      "100%|██████████████████████████████████████| 2000/2000 [00:10<00:00, 185.36it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 2000\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    for r in tqdm(range(R)):\n",
    "        Yexp,T,Xab = fn_generate_data_sele(tau,N,p,corr)   \n",
    "        Yt = Yexp[np.where(T==1)[0],:]\n",
    "        Yc = Yexp[np.where(T==0)[0],:]\n",
    "        tauhat,se_tauhat = fn_tauhat_means(Yt,Yc)\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "        \n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "688adb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=0.00884967405428685, RMSE=0.20156559080701186, size=0.06\n",
      "N=1000: bias=0.001100311161735364, RMSE=0.06257440705402993, size=0.05\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1062aef",
   "metadata": {},
   "source": [
    "## · Example of a real-life situation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9175b159",
   "metadata": {},
   "source": [
    "X: earings\n",
    "\n",
    "T: motivated characteristic\n",
    "\n",
    "Y: take a job training programme \n",
    "\n",
    "I want to study the effect of motivated characteristic on the possibility of taking a job training  programme .If I conditional on earnings, then I will introduce selection bias because people selected into different earning levels denpend on motivated characteristic and taking a job training programme or not. Therefore, motivated characteristic and taking a job training  programme become no longer independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b91ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
